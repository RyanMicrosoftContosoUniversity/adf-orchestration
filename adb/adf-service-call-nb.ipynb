{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e66b20f0-a40a-44e1-8542-1a4112ed1749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-mgmt-datafactory in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ce4223d5-2153-4889-9124-c421184762fd/lib/python3.11/site-packages (9.2.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.11/site-packages (from azure-mgmt-datafactory) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /databricks/python3/lib/python3.11/site-packages (from azure-mgmt-datafactory) (4.10.0)\n",
      "Requirement already satisfied: azure-common>=1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ce4223d5-2153-4889-9124-c421184762fd/lib/python3.11/site-packages (from azure-mgmt-datafactory) (1.1.28)\n",
      "Requirement already satisfied: azure-mgmt-core>=1.3.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ce4223d5-2153-4889-9124-c421184762fd/lib/python3.11/site-packages (from azure-mgmt-datafactory) (1.5.0)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in /databricks/python3/lib/python3.11/site-packages (from azure-mgmt-core>=1.3.2->azure-mgmt-datafactory) (1.32.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in /databricks/python3/lib/python3.11/site-packages (from azure-core>=1.31.0->azure-mgmt-core>=1.3.2->azure-mgmt-datafactory) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core>=1.31.0->azure-mgmt-core>=1.3.2->azure-mgmt-datafactory) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-mgmt-core>=1.3.2->azure-mgmt-datafactory) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-mgmt-core>=1.3.2->azure-mgmt-datafactory) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-mgmt-core>=1.3.2->azure-mgmt-datafactory) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-mgmt-core>=1.3.2->azure-mgmt-datafactory) (2023.7.22)\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pip install azure-mgmt-datafactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16804462-1719-4082-a3c1-7c001758ee3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-identity\n",
      "  Downloading azure_identity-1.23.0-py3-none-any.whl.metadata (81 kB)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in /databricks/python3/lib/python3.11/site-packages (from azure-identity) (1.32.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /databricks/python3/lib/python3.11/site-packages (from azure-identity) (41.0.3)\n",
      "Collecting msal>=1.30.0 (from azure-identity)\n",
      "  Downloading msal-1.32.3-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity)\n",
      "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /databricks/python3/lib/python3.11/site-packages (from azure-identity) (4.10.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in /databricks/python3/lib/python3.11/site-packages (from azure-core>=1.31.0->azure-identity) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core>=1.31.0->azure-identity) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.11/site-packages (from cryptography>=2.5->azure-identity) (1.15.1)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/lib/python3/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity) (2.3.0)\n",
      "Requirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (2023.7.22)\n",
      "Downloading azure_identity-1.23.0-py3-none-any.whl (186 kB)\n",
      "Downloading msal-1.32.3-py3-none-any.whl (115 kB)\n",
      "Downloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: msal, msal-extensions, azure-identity\n",
      "Successfully installed azure-identity-1.23.0 msal-1.32.3 msal-extensions-1.3.1\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pip install azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0c4a0c3-9f98-410c-9a76-d2ec8625c464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b439ebeb-17e1-494e-9e28-a20ea4c85c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import os\n",
    "from typing import Optional, Dict, List, Any\n",
    "\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.mgmt.datafactory import DataFactoryManagementClient\n",
    "\n",
    "\n",
    "class ADFService:\n",
    "    \"\"\"Service for interacting with Azure Data Factory.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        try:\n",
    "            self.subscription_id = dbutils.secrets.get(scope=\"adf-pipeline\", key=\"subscription-id\")\n",
    "            self.resource_group = dbutils.secrets.get('adf-pipeline', key='resource-group')\n",
    "            self.factory_name = dbutils.secrets.get('adf-pipeline', key='factory-name')\n",
    "            tenant_id = dbutils.secrets.get('adf-pipeline', key='fuam-spn-tenant-id')\n",
    "            client_id = dbutils.secrets.get('adf-pipeline', key='adf-automation-spn-client-id')\n",
    "            client_secret = dbutils.secrets.get('adf-pipeline', key='adf-automation-spn-client-secret')\n",
    "\n",
    "        except KeyError as e:\n",
    "            raise EnvironmentError(f\"Missing required environment variable: {e}\")\n",
    "\n",
    "        credential = ClientSecretCredential(\n",
    "            tenant_id=tenant_id,\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "        )\n",
    "\n",
    "        self.client = DataFactoryManagementClient(credential, self.subscription_id)\n",
    "\n",
    "    def trigger_pipeline(self, pipeline_name: str, parameters: Optional[Dict[str, str]] = None) -> str:\n",
    "        \"\"\"Trigger an existing Azure Data Factory pipeline.\n",
    "\n",
    "        Args:\n",
    "            pipeline_name: Name of the pipeline to run.\n",
    "            parameters: Optional parameters to pass to the pipeline.\n",
    "\n",
    "        Returns:\n",
    "            The run ID of the triggered pipeline run.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            run_response = self.client.pipelines.create_run(\n",
    "                resource_group_name=self.resource_group,\n",
    "                factory_name=self.factory_name,\n",
    "                pipeline_name=pipeline_name,\n",
    "                parameters=parameters or {},\n",
    "            )\n",
    "            return run_response.run_id\n",
    "        except Exception as e:\n",
    "            print(f\"Error triggering pipeline '{pipeline_name}': {e}\")\n",
    "\n",
    "    def get_pipeline_run_status(self, run_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get the status of a pipeline run.\n",
    "\n",
    "        Args:\n",
    "            run_id: The run ID returned by trigger_pipeline.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing status information about the pipeline run.\n",
    "        \"\"\"\n",
    "        run_response = self.client.pipeline_runs.get(\n",
    "            resource_group_name=self.resource_group,\n",
    "            factory_name=self.factory_name,\n",
    "            run_id=run_id,\n",
    "        )\n",
    "        return {\n",
    "            \"run_id\": run_response.run_id,\n",
    "            \"pipeline_name\": run_response.pipeline_name,\n",
    "            \"status\": run_response.status,\n",
    "            \"start_time\": run_response.run_start,\n",
    "            \"end_time\": run_response.run_end,\n",
    "            \"duration_in_ms\": run_response.run_duration or 0,\n",
    "            \"parameters\": run_response.parameters,\n",
    "        }\n",
    "\n",
    "    def list_pipelines(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"List all pipelines in the Azure Data Factory.\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries containing pipeline information.\n",
    "        \"\"\"\n",
    "        pipelines = self.client.pipelines.list_by_factory(\n",
    "            resource_group_name=self.resource_group,\n",
    "            factory_name=self.factory_name,\n",
    "        )\n",
    "\n",
    "        result = []\n",
    "        for pipeline in pipelines:\n",
    "            result.append(\n",
    "                {\n",
    "                    \"id\": pipeline.id,\n",
    "                    \"name\": pipeline.name,\n",
    "                    \"type\": pipeline.type,\n",
    "                    \"description\": pipeline.description,\n",
    "                    \"folder_name\": pipeline.folder.name if pipeline.folder else None,\n",
    "                }\n",
    "            )\n",
    "        return result\n",
    "\n",
    "    def cancel_pipeline_run(self, run_id: str) -> None:\n",
    "        \"\"\"Cancel a pipeline run.\n",
    "\n",
    "        Args:\n",
    "            run_id: The run ID of the pipeline run to cancel.\n",
    "        \"\"\"\n",
    "        self.client.pipeline_runs.cancel(\n",
    "            resource_group_name=self.resource_group,\n",
    "            factory_name=self.factory_name,\n",
    "            run_id=run_id,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac52b3ca-a92b-4290-9d5b-f2fbb519683b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.mgmt.datafactory._data_factory_management_client.DataFactoryManagementClient at 0x7f87e6223310>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service = ADFService()\n",
    "\n",
    "service.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1652b411-ae76-46f3-9142-1eca5f5e34ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 pipelines:\n",
      "1. pl_orchestration_recipe_1\n",
      "2. pl_orchestration_recipe_5_child\n",
      "3. pl_orchestration_recipe_4\n",
      "4. pl_orchestration_recipe_2\n",
      "5. pl_orchestration_recipe_3\n",
      "6. pl_orchestration_recipe_5_parent\n",
      "7. pl_api_ingestion_to_synapse - Pipeline to ingest from and API and sink to Synapse\n",
      "\n",
      "8. pl_orchestration_recipe_7_trigger\n",
      "9. plRunPythonOnSHIR\n",
      "10. plDICOM_Ingest - Ingest DICOM files from source VM\n",
      "11. plCopyData - Example Pipeline to Copy Data from one location to another.\n",
      "\n",
      "Parameterize Source and Sink based on input\n"
     ]
    }
   ],
   "source": [
    "pipelines = service.list_pipelines()\n",
    "\n",
    "\n",
    "print(f\"Found {len(pipelines)} pipelines:\")\n",
    "for i, pipeline in enumerate(pipelines, 1):\n",
    "    folder = f\" (folder: {pipeline['folder_name']})\" if pipeline['folder_name'] else \"\"\n",
    "    desc = f\" - {pipeline['description']}\" if pipeline['description'] else \"\"\n",
    "    print(f\"{i}. {pipeline['name']}{folder}{desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c839de-2c88-4704-ac39-fc1875499e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline run triggered with ID: aecb6bd0-4085-11f0-af6b-7c1e52cc27eb\n"
     ]
    }
   ],
   "source": [
    "# trigger pipeline run\n",
    "pipeline_name = 'plCopyData'\n",
    "parameters = {'file': 'SalesLTProduct.csv', 'sourcePath': 'wwi'}\n",
    "\n",
    "run_id = service.trigger_pipeline(pipeline_name, parameters)\n",
    "\n",
    "print(f\"Pipeline run triggered with ID: {run_id}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "adf-service-call-nb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
